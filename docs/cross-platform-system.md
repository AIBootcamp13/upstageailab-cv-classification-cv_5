# CV-Classify í¬ë¡œìŠ¤ í”Œë«í¼ ì‹œìŠ¤í…œ ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

CV-ClassifyëŠ” ë‹¤ì–‘í•œ ìš´ì˜ì²´ì œì™€ í•˜ë“œì›¨ì–´ í™˜ê²½ì—ì„œ ì¼ê´€ëœ ì„±ëŠ¥ì„ ì œê³µí•˜ë„ë¡ ì„¤ê³„ëœ í¬ë¡œìŠ¤ í”Œë«í¼ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ì´ ë¬¸ì„œëŠ” ê° í”Œë«í¼ë³„ íŠ¹ì§•, ìµœì í™” ì „ëµ, ì„¤ì¹˜ ë° ì„¤ì • ë°©ë²•ì„ ìƒì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ–¥ï¸ ì§€ì› í”Œë«í¼

### ì™„ì „ ì§€ì› í”Œë«í¼

| í”Œë«í¼ | ë²„ì „ | GPU ì§€ì› | íŒ¨í‚¤ì§€ ê´€ë¦¬ì | ìƒíƒœ |
|--------|------|----------|---------------|------|
| **macOS** | 10.15+ | Apple Silicon MPS | Homebrew | âœ… ì™„ì „ ì§€ì› |
| **Ubuntu** | 20.04/22.04 LTS | NVIDIA CUDA | APT | âœ… ì™„ì „ ì§€ì› |
| **CentOS** | 7/8 | NVIDIA CUDA | YUM/DNF | âœ… ì™„ì „ ì§€ì› |
| **Windows WSL2** | Ubuntu 20.04+ | NVIDIA CUDA | APT | âœ… ì™„ì „ ì§€ì› |

### ë¶€ë¶„ ì§€ì› í”Œë«í¼

| í”Œë«í¼ | ì œí•œì‚¬í•­ | ê¶Œì¥ ëŒ€ì•ˆ |
|--------|----------|-----------|
| **Windows ë„¤ì´í‹°ë¸Œ** | ì…¸ ìŠ¤í¬ë¦½íŠ¸ ë¯¸ì§€ì› | WSL2 ì‚¬ìš© ê¶Œì¥ |
| **ê¸°íƒ€ Linux ë°°í¬íŒ** | íŒ¨í‚¤ì§€ ê´€ë¦¬ì ì°¨ì´ | ìˆ˜ë™ ì„¤ì¹˜ í•„ìš” |

---

## ğŸ”§ í”Œë«í¼ë³„ ì•„í‚¤í…ì²˜

### 1. macOS (Apple Silicon + Intel)

#### ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
macOS í™˜ê²½
â”œâ”€â”€ Homebrew íŒ¨í‚¤ì§€ ê´€ë¦¬
â”œâ”€â”€ Python 3.7+ (Homebrew)
â”œâ”€â”€ Apple Silicon GPU (MPS)
â”‚   â”œâ”€â”€ í†µí•© ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ Metal Performance Shaders
â”‚   â””â”€â”€ ìë™ ë©”ëª¨ë¦¬ ê´€ë¦¬
â””â”€â”€ Intel x86_64 í˜¸í™˜ì„±
```

#### í•˜ë“œì›¨ì–´ ìµœì í™”

**Apple Silicon (M1/M2/M3/M4)**:
```python
# MPS ìµœì í™” ì„¤ì •
device = torch.device('mps')
dataloader_config = {
    "pin_memory": False,    # í†µí•© ë©”ëª¨ë¦¬ë¡œ ë¶ˆí•„ìš”
    "num_workers": 0,       # ë©€í‹°í”„ë¡œì„¸ì‹± ì´ìŠˆ ë°©ì§€
    "persistent_workers": False
}

# ë©”ëª¨ë¦¬ ìµœì í™”
torch.mps.empty_cache()  # ë©”ëª¨ë¦¬ ì •ë¦¬
```

**Intel Mac**:
```python
# CPU ìµœì í™” ì„¤ì •  
device = torch.device('cpu')
torch.set_num_threads(8)  # CPU ì½”ì–´ í™œìš©
dataloader_config = {
    "pin_memory": False,
    "num_workers": 4
}
```

#### íŠ¹ë³„ ê³ ë ¤ì‚¬í•­

**ì¥ì **:
- í†µí•© ë©”ëª¨ë¦¬ë¡œ ëŒ€ìš©ëŸ‰ ëª¨ë¸ ì²˜ë¦¬ ê°€ëŠ¥
- ì €ì „ë ¥ ê³ ì„±ëŠ¥
- ì•ˆì •ì ì¸ ê°œë°œ í™˜ê²½

**ì œí•œì‚¬í•­**:
- MPSëŠ” ì¼ë¶€ PyTorch ì—°ì‚° ë¯¸ì§€ì›
- ë©€í‹°í”„ë¡œì„¸ì‹± DataLoader ì´ìŠˆ
- CUDA ì „ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„± ë¬¸ì œ

### 2. Ubuntu Linux

#### ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
Ubuntu í™˜ê²½
â”œâ”€â”€ APT íŒ¨í‚¤ì§€ ê´€ë¦¬
â”œâ”€â”€ Python 3.7+ (ì‹œìŠ¤í…œ/PPA)
â”œâ”€â”€ NVIDIA GPU (CUDA)
â”‚   â”œâ”€â”€ CUDA Toolkit
â”‚   â”œâ”€â”€ cuDNN ë¼ì´ë¸ŒëŸ¬ë¦¬
â”‚   â””â”€â”€ GPU ë©”ëª¨ë¦¬ ê´€ë¦¬
â””â”€â”€ ê³ ì„±ëŠ¥ ì»´í“¨íŒ… ìµœì í™”
```

#### í•˜ë“œì›¨ì–´ ìµœì í™”

**NVIDIA GPU**:
```python
# CUDA ìµœì í™” ì„¤ì •
device = torch.device('cuda')
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False

dataloader_config = {
    "pin_memory": True,     # GPU ë©”ëª¨ë¦¬ ì „ì†¡ ìµœì í™”
    "num_workers": 8,       # ë³‘ë ¬ ë°ì´í„° ë¡œë”©
    "persistent_workers": True,
    "prefetch_factor": 2
}

# í˜¼í•© ì •ë°€ë„ í•™ìŠµ
scaler = torch.cuda.amp.GradScaler()
```

**CPU ì „ìš©**:
```python
# CPU ìµœì í™” ì„¤ì •
device = torch.device('cpu')
torch.set_num_threads(min(os.cpu_count(), 16))

dataloader_config = {
    "pin_memory": False,
    "num_workers": min(os.cpu_count() // 2, 8)
}
```

#### íŠ¹ë³„ ê³ ë ¤ì‚¬í•­

**ì¥ì **:
- ìµœê³  ì„±ëŠ¥ì˜ CUDA ì§€ì›
- ëŒ€ë¶€ë¶„ì˜ ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì™„ì „ í˜¸í™˜
- ì„œë²„ í™˜ê²½ ìµœì í™”

**ì œí•œì‚¬í•­**:
- NVIDIA ë“œë¼ì´ë²„ ì˜ì¡´ì„±
- CUDA ì„¤ì¹˜ ë³µì¡ì„±

### 3. CentOS/RHEL

#### ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
CentOS í™˜ê²½
â”œâ”€â”€ YUM/DNF íŒ¨í‚¤ì§€ ê´€ë¦¬
â”œâ”€â”€ Python 3.7+ (EPEL/SCL)
â”œâ”€â”€ NVIDIA GPU (CUDA)
â”‚   â”œâ”€â”€ ì—”í„°í”„ë¼ì´ì¦ˆ ì§€ì›
â”‚   â””â”€â”€ ì¥ê¸° ì•ˆì •ì„±
â””â”€â”€ ì„œë²„ í™˜ê²½ ìµœì í™”
```

#### íŠ¹ë³„ ì„¤ì •

**Python ì„¤ì¹˜**:
```bash
# CentOS 7
sudo yum install epel-release
sudo yum install python3 python3-pip

# CentOS 8
sudo dnf install python3 python3-pip
```

**CUDA ì„¤ì¹˜**:
```bash
# NVIDIA ë¦¬í¬ì§€í† ë¦¬ ì¶”ê°€
sudo dnf config-manager --add-repo \
    https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/cuda-rhel8.repo

sudo dnf install cuda
```

### 4. Windows WSL2

#### ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
Windows WSL2 í™˜ê²½
â”œâ”€â”€ Ubuntu 20.04+ ì„œë¸Œì‹œìŠ¤í…œ
â”œâ”€â”€ Windows GPU ê³µìœ 
â”œâ”€â”€ NVIDIA GPU (CUDA on WSL)
â”‚   â”œâ”€â”€ Windows NVIDIA ë“œë¼ì´ë²„
â”‚   â”œâ”€â”€ WSL2 CUDA ì§€ì›
â”‚   â””â”€â”€ í•˜ì´ë¸Œë¦¬ë“œ ë©”ëª¨ë¦¬ ê´€ë¦¬
â””â”€â”€ íŒŒì¼ ì‹œìŠ¤í…œ ì—°ë™
```

#### ì„¤ì • ìš”êµ¬ì‚¬í•­

**WSL2 CUDA ì„¤ì •**:
1. Windows NVIDIA ë“œë¼ì´ë²„ (471.41+)
2. WSL2 ì»¤ë„ ì—…ë°ì´íŠ¸
3. Ubuntu WSLì—ì„œ CUDA Toolkit ì„¤ì¹˜

```bash
# WSL2ì—ì„œ CUDA ì„¤ì¹˜
wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin
sudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600
sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/7fa2af80.pub
sudo add-apt-repository "deb https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/ /"
sudo apt-get update
sudo apt-get install cuda
```

---

## âš™ï¸ ìë™ í”Œë«í¼ ê°ì§€ ì‹œìŠ¤í…œ

### platform_utils.shì˜ ê°ì§€ ë¡œì§

```bash
detect_platform() {
    case "$(uname -s)" in
        Darwin)
            echo "macos"
            ;;
        Linux)
            if [ -f /etc/lsb-release ] || [ -f /etc/debian_version ]; then
                echo "ubuntu"
            elif [ -f /etc/redhat-release ] || [ -f /etc/centos-release ]; then
                echo "centos"
            else
                echo "linux"
            fi
            ;;
        CYGWIN*|MINGW*|MSYS*)
            echo "windows"
            ;;
        *)
            echo "unknown"
            ;;
    esac
}
```

### device_utils.pyì˜ ë””ë°”ì´ìŠ¤ ê°ì§€

```python
def get_optimal_device():
    """í¬ë¡œìŠ¤ í”Œë«í¼ ìµœì í™”ëœ ë””ë°”ì´ìŠ¤ ê°ì§€"""
    
    # 1. CUDA í™•ì¸ (ìµœìš°ì„ )
    if torch.cuda.is_available():
        device = torch.device('cuda')
        return device, "CUDA"
    
    # 2. MPS í™•ì¸ (macOS Apple Silicon)
    elif torch.backends.mps.is_available():
        device = torch.device('mps')
        return device, "MPS"
    
    # 3. CPU fallback
    else:
        device = torch.device('cpu')
        return device, "CPU"
```

---

## ğŸ“¦ í”Œë«í¼ë³„ ì„¤ì¹˜ ê°€ì´ë“œ

### 1. macOS ì„¤ì¹˜

#### ìë™ ì„¤ì¹˜ (ê¶Œì¥)

```bash
# ë¦¬í¬ì§€í† ë¦¬ í´ë¡ 
git clone <repository-url>
cd cv-classify

# ìë™ ì„¤ì • ì‹¤í–‰
chmod +x setup_macos.sh
./setup_macos.sh

# ë˜ëŠ” í†µí•© ì„¤ì •
chmod +x setup.sh
./setup.sh
```

#### ìˆ˜ë™ ì„¤ì¹˜

**1ë‹¨ê³„: Homebrew ì„¤ì¹˜**
```bash
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```

**2ë‹¨ê³„: í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜**
```bash
brew install python3 git screen tmux
```

**3ë‹¨ê³„: Python ì˜ì¡´ì„± ì„¤ì¹˜**
```bash
pip3 install -r requirements-macos.txt
```

### 2. Ubuntu ì„¤ì¹˜

#### ìë™ ì„¤ì¹˜ (ê¶Œì¥)

```bash
# ë¦¬í¬ì§€í† ë¦¬ í´ë¡ 
git clone <repository-url>
cd cv-classify

# ìë™ ì„¤ì • ì‹¤í–‰
chmod +x setup_ubuntu.sh
./setup_ubuntu.sh

# ë˜ëŠ” í†µí•© ì„¤ì •
chmod +x setup.sh
./setup.sh
```

#### ìˆ˜ë™ ì„¤ì¹˜

**1ë‹¨ê³„: ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸**
```bash
sudo apt-get update
sudo apt-get upgrade -y
```

**2ë‹¨ê³„: í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜**
```bash
sudo apt-get install -y \
    python3 python3-pip python3-venv \
    git curl wget screen tmux \
    build-essential
```

**3ë‹¨ê³„: NVIDIA GPU ì„¤ì • (ì„ íƒì‚¬í•­)**
```bash
# NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜
sudo apt-get install nvidia-driver-470

# CUDA ì„¤ì¹˜
wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run
sudo sh cuda_11.8.0_520.61.05_linux.run
```

**4ë‹¨ê³„: Python ì˜ì¡´ì„± ì„¤ì¹˜**
```bash
# GPU ë²„ì „
pip3 install -r requirements-ubuntu-gpu.txt

# CPU ë²„ì „
pip3 install -r requirements-ubuntu-cpu.txt
```

### 3. CentOS ì„¤ì¹˜

#### ìë™ ì„¤ì¹˜

```bash
# ë¦¬í¬ì§€í† ë¦¬ í´ë¡ 
git clone <repository-url>
cd cv-classify

# ìë™ ì„¤ì • ì‹¤í–‰ (CentOSëŠ” í†µí•© ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©)
chmod +x setup.sh
./setup.sh
```

#### ìˆ˜ë™ ì„¤ì¹˜

**CentOS 7**:
```bash
# EPEL ì €ì¥ì†Œ ì¶”ê°€
sudo yum install epel-release

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
sudo yum install python3 python3-pip git screen tmux

# Python ì˜ì¡´ì„± ì„¤ì¹˜
pip3 install -r requirements.txt
```

**CentOS 8**:
```bash
# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
sudo dnf install python3 python3-pip git screen tmux

# Python ì˜ì¡´ì„± ì„¤ì¹˜
pip3 install -r requirements.txt
```

### 4. Windows WSL2 ì„¤ì¹˜

#### 1ë‹¨ê³„: WSL2 ì„¤ì •

```powershell
# Windows PowerShell (ê´€ë¦¬ì ê¶Œí•œ)
wsl --install -d Ubuntu-20.04
```

#### 2ë‹¨ê³„: Ubuntu ì„¤ì •

WSL2 Ubuntu í„°ë¯¸ë„ì—ì„œ:
```bash
# ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸
sudo apt-get update && sudo apt-get upgrade -y

# CV-Classify ì„¤ì¹˜ (Ubuntuì™€ ë™ì¼)
git clone <repository-url>
cd cv-classify
chmod +x setup.sh
./setup.sh
```

#### 3ë‹¨ê³„: CUDA on WSL ì„¤ì • (ì„ íƒì‚¬í•­)

```bash
# WSL2 CUDA ì„¤ì¹˜
sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/7fa2af80.pub
sudo add-apt-repository "deb https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/ /"
sudo apt-get update
sudo apt-get install cuda
```

---

## ğŸ”„ í”Œë«í¼ ê°„ í˜¸í™˜ì„± ê´€ë¦¬

### íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬

```python
# config.pyì—ì„œ í¬ë¡œìŠ¤ í”Œë«í¼ ê²½ë¡œ ì²˜ë¦¬
from pathlib import Path

PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "data"
MODELS_DIR = PROJECT_ROOT / "models"

# ìë™ìœ¼ë¡œ í”Œë«í¼ë³„ êµ¬ë¶„ì ì‚¬ìš© (/ vs \)
```

### ìŠ¤í¬ë¦½íŠ¸ í˜¸í™˜ì„±

```bash
# menu.shì—ì„œ í¬ë¡œìŠ¤ í”Œë«í¼ ëª…ë ¹ì–´ ì²˜ë¦¬
clear_screen() {
    if command -v clear &> /dev/null; then
        clear
    else
        printf '\033[2J\033[H'  # ANSI ì´ìŠ¤ì¼€ì´í”„ ì‹œí€€ìŠ¤
    fi
}
```

### Python ëª…ë ¹ì–´ í†µì¼

```bash
# platform_utils.shì—ì„œ Python ëª…ë ¹ì–´ ìë™ ê°ì§€
detect_python() {
    if command -v python3 &> /dev/null; then
        echo "python3"
    elif command -v python &> /dev/null; then
        echo "python"
    else
        echo "python3"  # ê¸°ë³¸ê°’
    fi
}
```

---

## ğŸš€ ì„±ëŠ¥ ìµœì í™” ì „ëµ

### í”Œë«í¼ë³„ ìµœì í™” ë§¤íŠ¸ë¦­ìŠ¤

| í”Œë«í¼ | ë””ë°”ì´ìŠ¤ | pin_memory | num_workers | íŠ¹ë³„ ì„¤ì • |
|--------|----------|------------|-------------|-----------|
| **macOS Apple Silicon** | MPS | False | 0 | í†µí•© ë©”ëª¨ë¦¬ |
| **macOS Intel** | CPU | False | 4 | ë©€í‹°ìŠ¤ë ˆë”© |
| **Ubuntu CUDA** | CUDA | True | 8 | cuDNN ë²¤ì¹˜ë§ˆí‚¹ |
| **Ubuntu CPU** | CPU | False | 4-8 | OpenMP ìµœì í™” |
| **WSL2 CUDA** | CUDA | True | 6 | í•˜ì´ë¸Œë¦¬ë“œ ë©”ëª¨ë¦¬ |

### ìë™ ì„±ëŠ¥ ì¡°ì •

```python
def get_optimal_batch_size(device_type, model_name):
    """í”Œë«í¼ë³„ ìµœì  ë°°ì¹˜ í¬ê¸° ìë™ ê²°ì •"""
    
    base_batch_sizes = {
        "MPS": 32,      # Apple Silicon í†µí•© ë©”ëª¨ë¦¬
        "CUDA": 64,     # NVIDIA GPU ì „ìš© ë©”ëª¨ë¦¬
        "CPU": 16       # CPU ì œí•œëœ ë©”ëª¨ë¦¬
    }
    
    # ëª¨ë¸ë³„ ì¡°ì •
    model_multipliers = {
        "resnet34": 1.0,
        "resnet50": 0.8,
        "efficientnet_b0": 1.2,
        "efficientnet_b4": 0.6
    }
    
    base_size = base_batch_sizes.get(device_type, 16)
    multiplier = model_multipliers.get(model_name, 1.0)
    
    return int(base_size * multiplier)
```

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

```python
def monitor_memory_usage(device):
    """í”Œë«í¼ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
    
    if device.type == 'cuda':
        total = torch.cuda.get_device_properties(0).total_memory
        allocated = torch.cuda.memory_allocated(0)
        cached = torch.cuda.memory_reserved(0)
        
        return {
            'total': total / 1e9,
            'allocated': allocated / 1e9,
            'cached': cached / 1e9,
            'free': (total - allocated) / 1e9
        }
    
    elif device.type == 'mps':
        # Apple Silicon í†µí•© ë©”ëª¨ë¦¬
        import psutil
        mem = psutil.virtual_memory()
        return {
            'total': mem.total / 1e9,
            'available': mem.available / 1e9,
            'used': mem.used / 1e9,
            'percent': mem.percent
        }
    
    else:  # CPU
        import psutil
        mem = psutil.virtual_memory()
        return {
            'total': mem.total / 1e9,
            'available': mem.available / 1e9,
            'used': mem.used / 1e9,
            'percent': mem.percent
        }
```

---

## ğŸ› ï¸ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ

### ê³µí†µ ë¬¸ì œ í•´ê²°

#### 1. Python ëª…ë ¹ì–´ ì¸ì‹ ì˜¤ë¥˜

**ì¦ìƒ**:
```
python: command not found
python3: command not found
```

**í•´ê²°ì±…**:
```bash
# macOS
brew install python3

# Ubuntu
sudo apt-get install python3 python3-pip

# CentOS
sudo yum install python3 python3-pip
```

#### 2. ê¶Œí•œ ì˜¤ë¥˜

**ì¦ìƒ**:
```
Permission denied: ./setup.sh
```

**í•´ê²°ì±…**:
```bash
chmod +x setup.sh
chmod +x menu.sh
chmod +x scripts/*.sh
```

#### 3. íŒ¨í‚¤ì§€ ê´€ë¦¬ì ì˜¤ë¥˜

**ì¦ìƒ**:
```
brew: command not found (macOS)
apt-get: command not found (Ubuntu)
```

**í•´ê²°ì±…**:
```bash
# macOS - Homebrew ì„¤ì¹˜
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# Ubuntu - APT ì—…ë°ì´íŠ¸
sudo apt-get update

# CentOS - EPEL ì €ì¥ì†Œ ì¶”ê°€
sudo yum install epel-release
```

### í”Œë«í¼ë³„ íŠ¹ìˆ˜ ë¬¸ì œ

#### macOS íŠ¹ìˆ˜ ë¬¸ì œ

**1. MPS ë””ë°”ì´ìŠ¤ ì˜¤ë¥˜**

**ì¦ìƒ**:
```
RuntimeError: MPS backend out of memory
```

**í•´ê²°ì±…**:
```python
# ë©”ëª¨ë¦¬ ì •ë¦¬
if torch.backends.mps.is_available():
    torch.mps.empty_cache()

# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
BATCH_SIZE = 16  # ê¸°ë³¸ 32ì—ì„œ 16ìœ¼ë¡œ ê°ì†Œ
```

**2. Rosetta í˜¸í™˜ì„± ë¬¸ì œ**

**ì¦ìƒ**:
```
ImportError: cannot import name '_C' from 'torch'
```

**í•´ê²°ì±…**:
```bash
# Apple Silicon ë„¤ì´í‹°ë¸Œ Python ì‚¬ìš©
arch -arm64 brew install python3
arch -arm64 pip3 install torch torchvision
```

**3. Xcode Command Line Tools ëˆ„ë½**

**ì¦ìƒ**:
```
xcrun: error: invalid active developer path
```

**í•´ê²°ì±…**:
```bash
xcode-select --install
```

#### Ubuntu íŠ¹ìˆ˜ ë¬¸ì œ

**1. CUDA ë²„ì „ ë¶ˆì¼ì¹˜**

**ì¦ìƒ**:
```
RuntimeError: CUDA error: no kernel image is available for execution
```

**í•´ê²°ì±…**:
```bash
# CUDA ë²„ì „ í™•ì¸
nvcc --version
nvidia-smi

# PyTorch ì¬ì„¤ì¹˜ (CUDA ë²„ì „ì— ë§ê²Œ)
pip3 uninstall torch torchvision
pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

**2. NVIDIA ë“œë¼ì´ë²„ ì¶©ëŒ**

**ì¦ìƒ**:
```
NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver
```

**í•´ê²°ì±…**:
```bash
# ë“œë¼ì´ë²„ ì¬ì„¤ì¹˜
sudo apt-get purge nvidia-*
sudo apt-get autoremove
sudo apt-get install nvidia-driver-470
sudo reboot
```

**3. CUDNN ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½**

**ì¦ìƒ**:
```
UserWarning: cuDNN is not available
```

**í•´ê²°ì±…**:
```bash
# cuDNN ì„¤ì¹˜
sudo apt-get install libcudnn8 libcudnn8-dev
```

#### Windows WSL2 íŠ¹ìˆ˜ ë¬¸ì œ

**1. WSL2 CUDA ì§€ì› ë¬¸ì œ**

**ì¦ìƒ**:
```
torch.cuda.is_available() returns False
```

**í•´ê²°ì±…**:
```bash
# WSL2 CUDA ì§€ì› í™•ì¸
ls /usr/lib/wsl/lib/

# CUDA í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
echo 'export PATH=/usr/local/cuda/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc
```

**2. íŒŒì¼ ì‹œìŠ¤í…œ ê¶Œí•œ ë¬¸ì œ**

**ì¦ìƒ**:
```
PermissionError: [Errno 13] Permission denied
```

**í•´ê²°ì±…**:
```bash
# WSL2ì—ì„œ Windows íŒŒì¼ ì‹œìŠ¤í…œ ì‚¬ìš© ì‹œ
# Linux íŒŒì¼ ì‹œìŠ¤í…œìœ¼ë¡œ í”„ë¡œì íŠ¸ ì´ë™
cp -r /mnt/c/cv-classify ~/cv-classify
cd ~/cv-classify
```

#### CentOS íŠ¹ìˆ˜ ë¬¸ì œ

**1. Python 3.7+ ì„¤ì¹˜ ë¬¸ì œ**

**ì¦ìƒ**:
```
No package python3.8 available
```

**í•´ê²°ì±…**:
```bash
# CentOS 7 - Software Collections ì‚¬ìš©
sudo yum install centos-release-scl
sudo yum install rh-python38
scl enable rh-python38 bash

# CentOS 8 - AppStream ì‚¬ìš©
sudo dnf install python38
```

**2. ì»´íŒŒì¼ëŸ¬ ë„êµ¬ ëˆ„ë½**

**ì¦ìƒ**:
```
error: Microsoft Visual C++ 14.0 is required
```

**í•´ê²°ì±…**:
```bash
# ê°œë°œ ë„êµ¬ ì„¤ì¹˜
sudo yum groupinstall "Development Tools"
sudo yum install python3-devel
```

---

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### í”Œë«í¼ë³„ ì„±ëŠ¥ ë¹„êµ

#### ë² ì´ìŠ¤ë¼ì¸ í…ŒìŠ¤íŠ¸ (ResNet34, 1 epoch)

| í”Œë«í¼ | í•˜ë“œì›¨ì–´ | ë°°ì¹˜ í¬ê¸° | ì‹œê°„ (ì´ˆ) | ë©”ëª¨ë¦¬ (GB) | ì„±ëŠ¥ ì ìˆ˜ |
|--------|----------|-----------|-----------|------------|-----------|
| **macOS M1 Pro** | 8ì½”ì–´ CPU + 14ì½”ì–´ GPU | 32 | 45 | 8.5 | â­â­â­â­â­ |
| **Ubuntu RTX 3080** | CUDA | 64 | 25 | 6.2 | â­â­â­â­â­ |
| **Ubuntu RTX 2060** | CUDA | 32 | 35 | 4.1 | â­â­â­â­ |
| **macOS Intel i7** | 8ì½”ì–´ CPU | 16 | 180 | 12.3 | â­â­â­ |
| **Ubuntu CPU (16ì½”ì–´)** | CPU ì „ìš© | 16 | 220 | 8.7 | â­â­â­ |
| **WSL2 RTX 3070** | CUDA on WSL | 32 | 30 | 5.8 | â­â­â­â­ |

#### ê³ ê¸‰ ì‹¤í—˜ í…ŒìŠ¤íŠ¸ (ResNet34, 50 epochs)

| í”Œë«í¼ | ì˜ˆìƒ ì‹œê°„ | ìµœì  ë°°ì¹˜ í¬ê¸° | ê¶Œì¥ ì„¤ì • |
|--------|-----------|----------------|-----------|
| **Apple Silicon** | 45ë¶„ | 32 | MPS + í†µí•© ë©”ëª¨ë¦¬ |
| **NVIDIA RTX 30xx** | 25ë¶„ | 64 | CUDA + í˜¼í•© ì •ë°€ë„ |
| **NVIDIA RTX 20xx** | 35ë¶„ | 32 | CUDA + í‘œì¤€ ì •ë°€ë„ |
| **ê³ ì„±ëŠ¥ CPU** | 3-4ì‹œê°„ | 16 | ë©€í‹°ìŠ¤ë ˆë”© + ìºì‹± |

### ìë™ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

```python
def run_performance_test():
    """í”Œë«í¼ë³„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    
    from device_utils import setup_training_device
    import time
    import torch
    
    device, device_type = setup_training_device()
    
    # í…ŒìŠ¤íŠ¸ ì„¤ì •
    batch_size = 32
    img_size = 224
    num_classes = 17
    
    # ëª¨ë¸ ìƒì„±
    model = torch.nn.Sequential(
        torch.nn.Conv2d(3, 64, 3, padding=1),
        torch.nn.ReLU(),
        torch.nn.AdaptiveAvgPool2d(1),
        torch.nn.Flatten(),
        torch.nn.Linear(64, num_classes)
    ).to(device)
    
    # ë”ë¯¸ ë°ì´í„°
    dummy_input = torch.randn(batch_size, 3, img_size, img_size).to(device)
    dummy_target = torch.randint(0, num_classes, (batch_size,)).to(device)
    
    # ì„±ëŠ¥ ì¸¡ì •
    model.train()
    optimizer = torch.optim.Adam(model.parameters())
    criterion = torch.nn.CrossEntropyLoss()
    
    # ì›Œë°ì—…
    for _ in range(5):
        output = model(dummy_input)
        loss = criterion(output, dummy_target)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
    
    # ì‹¤ì œ ì¸¡ì •
    start_time = time.time()
    for _ in range(100):
        output = model(dummy_input)
        loss = criterion(output, dummy_target)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
    
    end_time = time.time()
    
    # ê²°ê³¼ ì¶œë ¥
    total_time = end_time - start_time
    samples_per_second = (100 * batch_size) / total_time
    
    print(f"í”Œë«í¼: {device_type}")
    print(f"ë””ë°”ì´ìŠ¤: {device}")
    print(f"ì´ ì‹œê°„: {total_time:.2f}ì´ˆ")
    print(f"ì²˜ë¦¬ëŸ‰: {samples_per_second:.1f} samples/second")
    
    return {
        'device_type': device_type,
        'total_time': total_time,
        'throughput': samples_per_second
    }
```

---

## ğŸ”„ ë°°í¬ ë° ì´ì‹ì„±

### í™˜ê²½ ë…ë¦½ì  ì‹¤í–‰

#### Docker ì§€ì› (ê³„íš ì¤‘)

```dockerfile
# Dockerfile.cuda (NVIDIA GPU ì§€ì›)
FROM nvidia/cuda:11.8-devel-ubuntu20.04

RUN apt-get update && apt-get install -y \
    python3 python3-pip git

COPY requirements-ubuntu-gpu.txt .
RUN pip3 install -r requirements-ubuntu-gpu.txt

COPY . /cv-classify
WORKDIR /cv-classify

CMD ["python3", "codes/train_with_wandb.py"]
```

```dockerfile
# Dockerfile.cpu (CPU ì „ìš©)
FROM python:3.9-slim

RUN apt-get update && apt-get install -y git

COPY requirements-ubuntu-cpu.txt .
RUN pip install -r requirements-ubuntu-cpu.txt

COPY . /cv-classify
WORKDIR /cv-classify

CMD ["python3", "codes/train_with_wandb.py"]
```

#### ê°€ìƒ í™˜ê²½ ê´€ë¦¬

```bash
# Python venv ì‚¬ìš©
python3 -m venv cv-classify-env
source cv-classify-env/bin/activate  # Linux/macOS
# cv-classify-env\Scripts\activate  # Windows

pip install -r requirements.txt
```

```bash
# Conda í™˜ê²½ ì‚¬ìš©
conda create -n cv-classify python=3.9
conda activate cv-classify
pip install -r requirements.txt
```

### ì„¤ì • í¬í„°ë¹Œë¦¬í‹°

#### í™˜ê²½ ë³€ìˆ˜ í†µí•© ê´€ë¦¬

```bash
# .env.template íŒŒì¼
WANDB_API_KEY=your_wandb_api_key_here
WANDB_PROJECT=cv-classification
WANDB_ENTITY=your_wandb_entity
WANDB_MODE=online

# í”Œë«í¼ë³„ ì˜¤ë²„ë¼ì´ë“œ
MACOS_OPTIMIZATION=true
CUDA_OPTIMIZATION=true
CPU_THREADS=auto
```

#### í”Œë«í¼ë³„ ì„¤ì • í”„ë¡œíŒŒì¼

```python
# config.pyì—ì„œ í”Œë«í¼ë³„ ì„¤ì • í”„ë¡œíŒŒì¼
PLATFORM_PROFILES = {
    "macos_m1": {
        "device_preference": ["mps", "cpu"],
        "dataloader_workers": 0,
        "pin_memory": False,
        "batch_size_multiplier": 1.0
    },
    "ubuntu_cuda": {
        "device_preference": ["cuda", "cpu"],
        "dataloader_workers": 8,
        "pin_memory": True,
        "batch_size_multiplier": 1.5
    },
    "cpu_only": {
        "device_preference": ["cpu"],
        "dataloader_workers": 4,
        "pin_memory": False,
        "batch_size_multiplier": 0.5
    }
}
```

---

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…

### í”Œë«í¼ë³„ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§

```python
def get_system_metrics():
    """í”Œë«í¼ë³„ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
    
    import psutil
    import platform
    
    metrics = {
        'platform': platform.system(),
        'architecture': platform.machine(),
        'cpu_count': psutil.cpu_count(),
        'cpu_percent': psutil.cpu_percent(interval=1),
        'memory': {
            'total': psutil.virtual_memory().total / 1e9,
            'available': psutil.virtual_memory().available / 1e9,
            'percent': psutil.virtual_memory().percent
        }
    }
    
    # GPU ì •ë³´ ì¶”ê°€
    if torch.cuda.is_available():
        metrics['gpu'] = {
            'name': torch.cuda.get_device_name(0),
            'memory_total': torch.cuda.get_device_properties(0).total_memory / 1e9,
            'memory_allocated': torch.cuda.memory_allocated(0) / 1e9
        }
    elif torch.backends.mps.is_available():
        metrics['gpu'] = {
            'name': 'Apple Silicon',
            'type': 'MPS'
        }
    
    return metrics
```

### ë¡œê·¸ í†µí•© ê´€ë¦¬

```python
def setup_cross_platform_logging():
    """í¬ë¡œìŠ¤ í”Œë«í¼ ë¡œê¹… ì„¤ì •"""
    
    import logging
    import os
    from datetime import datetime
    
    # í”Œë«í¼ë³„ ë¡œê·¸ ë””ë ‰í† ë¦¬
    if os.name == 'nt':  # Windows
        log_dir = os.path.expanduser('~\\AppData\\Local\\cv-classify\\logs')
    else:  # Unix-like
        log_dir = os.path.expanduser('~/.cv-classify/logs')
    
    os.makedirs(log_dir, exist_ok=True)
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ ë¡œê·¸ íŒŒì¼
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"cv_classify_{timestamp}.log")
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    return logging.getLogger(__name__)
```

---

## ğŸ¯ ìµœì í™” ê¶Œì¥ì‚¬í•­

### í”Œë«í¼ë³„ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

#### macOS ìµœì í™”

```python
# macOS íŠ¹í™” ì„¤ì •
if platform.system() == 'Darwin':
    # MPS ì‚¬ìš© ì‹œ ë°°ì¹˜ í¬ê¸° ì¡°ì •
    if torch.backends.mps.is_available():
        BATCH_SIZE = min(BATCH_SIZE, 32)
        NUM_WORKERS = 0  # ë©€í‹°í”„ë¡œì„¸ì‹± ë¬¸ì œ ë°©ì§€
    
    # ë©”ëª¨ë¦¬ ê´€ë¦¬
    import gc
    gc.collect()
    if torch.backends.mps.is_available():
        torch.mps.empty_cache()
```

#### Ubuntu ìµœì í™”

```python
# Ubuntu íŠ¹í™” ì„¤ì •
if platform.system() == 'Linux':
    # CUDA ì‚¬ìš© ì‹œ ì„±ëŠ¥ ìµœì í™”
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        
        # í˜¼í•© ì •ë°€ë„ í•™ìŠµ
        from torch.cuda.amp import autocast, GradScaler
        scaler = GradScaler()
        
        # ë°°ì¹˜ í¬ê¸° ì¦ê°€
        BATCH_SIZE = min(BATCH_SIZE * 2, 128)
```

#### ë²”ìš© CPU ìµœì í™”

```python
# CPU ì „ìš© ìµœì í™”
if not (torch.cuda.is_available() or torch.backends.mps.is_available()):
    import multiprocessing
    
    # CPU ìŠ¤ë ˆë“œ ìˆ˜ ìµœì í™”
    num_cores = multiprocessing.cpu_count()
    torch.set_num_threads(min(num_cores, 16))
    
    # ë°°ì¹˜ í¬ê¸° ì¡°ì •
    BATCH_SIZE = min(BATCH_SIZE, 16)
    
    # ë°ì´í„° ë¡œë”© ìµœì í™”
    NUM_WORKERS = min(num_cores // 2, 8)
```

### ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±

```python
def optimize_memory_usage():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
    
    # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…
    if hasattr(model, 'gradient_checkpointing_enable'):
        model.gradient_checkpointing_enable()
    
    # í˜¼í•© ì •ë°€ë„ í•™ìŠµ (CUDA)
    if torch.cuda.is_available():
        model = model.half()  # FP16 ì‚¬ìš©
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    import gc
    gc.collect()
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    elif torch.backends.mps.is_available():
        torch.mps.empty_cache()
```

---

## ğŸ”š ê²°ë¡ 

CV-Classifyì˜ í¬ë¡œìŠ¤ í”Œë«í¼ ì‹œìŠ¤í…œì€ ë‹¤ì–‘í•œ í•˜ë“œì›¨ì–´ì™€ ìš´ì˜ì²´ì œì—ì„œ ì¼ê´€ëœ ì„±ëŠ¥ê³¼ ì‚¬ìš©ì ê²½í—˜ì„ ì œê³µí•©ë‹ˆë‹¤. 

**ì£¼ìš” ì¥ì **:
- **ìë™ ìµœì í™”**: í”Œë«í¼ë³„ í•˜ë“œì›¨ì–´ì— ë§ëŠ” ìë™ ì„¤ì •
- **ì¼ê´€ëœ ì¸í„°í˜ì´ìŠ¤**: ëª¨ë“  í”Œë«í¼ì—ì„œ ë™ì¼í•œ ì‚¬ìš©ë²•
- **ì„±ëŠ¥ ìµœì í™”**: ê° í”Œë«í¼ì˜ íŠ¹ì„±ì„ ì‚´ë¦° ìµœì í™”
- **ë¬¸ì œ í•´ê²°**: í”Œë«í¼ë³„ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ ì œê³µ

**ì§€ì› ìš°ì„ ìˆœìœ„**:
1. **Tier 1**: macOS Apple Silicon, Ubuntu CUDA
2. **Tier 2**: macOS Intel, Windows WSL2
3. **Tier 3**: CentOS, ê¸°íƒ€ Linux ë°°í¬íŒ

ì´ ê°€ì´ë“œë¥¼ í†µí•´ ì–´ë–¤ í™˜ê²½ì—ì„œë“  CV-Classifyë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""
OCR í†µí•© ë¬¸ì„œ ë¶„ë¥˜ ëª¨ë¸ - ì‹¤ì œ ì ìš© ë²„ì „
ê¸°ì¡´ train_with_wandb.pyë¥¼ ê¸°ë°˜ìœ¼ë¡œ OCR ê¸°ëŠ¥ ì¶”ê°€
"""

import os
import time
import pickle
import sys
from pathlib import Path
from datetime import datetime

import timm
import torch
import albumentations as A
import pandas as pd
import numpy as np
import torch.nn as nn
from albumentations.pytorch import ToTensorV2
from torch.optim import Adam
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import CosineAnnealingLR
from PIL import Image
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

# ê¸°ì¡´ ëª¨ë“ˆ ì„í¬íŠ¸
from config import (
    WANDB_CONFIG, EXPERIMENT_CONFIG, DATA_CONFIG, 
    get_wandb_config, get_experiment_name, validate_config
)
from wandb_utils import (
    init_wandb, log_metrics, log_model_info, log_system_info,
    finish_run, create_run_name, log_confusion_matrix
)
from device_utils import setup_training_device, get_dataloader_config

# OCR ê´€ë ¨ ì„í¬íŠ¸
try:
    import easyocr
    OCR_AVAILABLE = True
    OCR_BACKEND = 'easyocr'
except ImportError:
    try:
        import pytesseract
        OCR_AVAILABLE = True
        OCR_BACKEND = 'tesseract'
    except ImportError:
        OCR_AVAILABLE = False
        OCR_BACKEND = None

class OCRExtractor:
    """ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    
    def __init__(self, backend='easyocr', cache_dir='data/ocr_cache'):
        self.backend = backend
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        if backend == 'easyocr' and OCR_AVAILABLE:
            print("ğŸ”¤ EasyOCR ì´ˆê¸°í™” ì¤‘...")
            self.reader = easyocr.Reader(['ko', 'en'], gpu=torch.cuda.is_available())
        elif backend == 'tesseract' and OCR_AVAILABLE:
            print("ğŸ”¤ Tesseract ì´ˆê¸°í™” ì¤‘...")
            self.tesseract_config = '--psm 6 -l kor+eng'
    
    def extract_text(self, image_path):
        """ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìºì‹± ì§€ì›)"""
        image_name = Path(image_path).name
        cache_file = self.cache_dir / f"{image_name}.txt"
        
        # ìºì‹œëœ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        if cache_file.exists():
            with open(cache_file, 'r', encoding='utf-8') as f:
                return f.read()
        
        # OCR ì‹¤í–‰
        text = self._extract_text_from_image(image_path)
        
        # ê²°ê³¼ ìºì‹±
        with open(cache_file, 'w', encoding='utf-8') as f:
            f.write(text)
        
        return text
    
    def _extract_text_from_image(self, image_path):
        """ì‹¤ì œ OCR ì‹¤í–‰"""
        try:
            if self.backend == 'easyocr':
                results = self.reader.readtext(image_path)
                # ì‹ ë¢°ë„ 30% ì´ìƒì¸ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©
                texts = [result[1] for result in results if result[2] > 0.3]
                return ' '.join(texts)
            
            elif self.backend == 'tesseract':
                image = Image.open(image_path)
                text = pytesseract.image_to_string(image, config=self.tesseract_config)
                return text.strip()
                
        except Exception as e:
            print(f"OCR ì‹¤íŒ¨ {image_path}: {e}")
            return ""
        
        return ""

class EnhancedImageDataset(Dataset):
    """OCR í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ í–¥ìƒëœ ë°ì´í„°ì…‹"""
    
    def __init__(self, csv_path, image_dir, transform=None, is_train=True, use_ocr=False, text_features=None):
        self.df = pd.read_csv(csv_path)
        self.image_dir = image_dir
        self.transform = transform
        self.is_train = is_train
        self.use_ocr = use_ocr
        self.text_features = text_features
        
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        
        if self.is_train:
            image_name, target = row['ID'], row['target']
        else:
            image_name, target = row['ID'], 0
            
        image_path = os.path.join(self.image_dir, image_name)
        image = np.array(Image.open(image_path))
        
        if self.transform:
            image = self.transform(image=image)['image']
        
        # OCR í…ìŠ¤íŠ¸ íŠ¹ì§•ì´ ìˆìœ¼ë©´ ì¶”ê°€
        if self.use_ocr and self.text_features is not None:
            text_feature = torch.FloatTensor(self.text_features[idx])
            return image, text_feature, target
        else:
            return image, target

class MultiModalModel(nn.Module):
    """ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ìœµí•© ëª¨ë¸"""
    
    def __init__(self, model_name='resnet34', num_classes=17, text_feature_dim=1000, use_ocr=False):
        super().__init__()
        
        self.use_ocr = use_ocr
        
        # ì´ë¯¸ì§€ ì¸ì½”ë”
        self.image_model = timm.create_model(
            model_name,
            pretrained=True,
            num_classes=num_classes if not use_ocr else 0  # OCR ì‚¬ìš©ì‹œ íŠ¹ì§•ë§Œ ì¶”ì¶œ
        )
        
        if use_ocr:
            # ë©€í‹°ëª¨ë‹¬ ì„¤ì •
            image_feature_dim = self.image_model.num_features
            
            # í…ìŠ¤íŠ¸ ì²˜ë¦¬ê¸°
            self.text_processor = nn.Sequential(
                nn.Linear(text_feature_dim, 512),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(512, 256),
                nn.ReLU(),
                nn.Dropout(0.2)
            )
            
            # ìœµí•© ë¶„ë¥˜ê¸°
            fusion_dim = image_feature_dim + 256
            self.classifier = nn.Sequential(
                nn.Linear(fusion_dim, 512),
                nn.ReLU(),
                nn.Dropout(0.4),
                nn.Linear(512, num_classes)
            )
    
    def forward(self, images, text_features=None):
        if self.use_ocr and text_features is not None:
            # ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬
            image_features = self.image_model(images)
            text_features = self.text_processor(text_features)
            
            # íŠ¹ì§• ìœµí•©
            fused_features = torch.cat([image_features, text_features], dim=1)
            return self.classifier(fused_features)
        else:
            # ì´ë¯¸ì§€ë§Œ ì‚¬ìš©
            return self.image_model(images)

def extract_and_cache_ocr_features(csv_path, image_dir, save_path=None, backend='easyocr'):
    """ì „ì²´ ë°ì´í„°ì…‹ì—ì„œ OCR íŠ¹ì§• ì¶”ì¶œ ë° ìºì‹±"""
    
    if save_path and Path(save_path).exists():
        print(f"ğŸ“ ìºì‹œëœ OCR íŠ¹ì§• ë¡œë“œ: {save_path}")
        with open(save_path, 'rb') as f:
            return pickle.load(f)
    
    print(f"ğŸ”¤ OCR íŠ¹ì§• ì¶”ì¶œ ì‹œì‘ ({backend})...")
    
    # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
    current_dir = Path.cwd()
    if current_dir.name == 'codes':
        # codes ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰ë˜ëŠ” ê²½ìš°
        project_root = current_dir.parent
    else:
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰ë˜ëŠ” ê²½ìš°
        project_root = current_dir
    
    cache_dir = project_root / 'data' / 'ocr_cache'
    ocr_extractor = OCRExtractor(backend=backend, cache_dir=str(cache_dir))
    df = pd.read_csv(csv_path)
    
    texts = []
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="OCR ì²˜ë¦¬"):
        image_path = os.path.join(image_dir, row['ID'])
        text = ocr_extractor.extract_text(image_path)
        texts.append(text if text.strip() else "ë¹ˆë¬¸ì„œ")
    
    # TF-IDF íŠ¹ì§•í™”
    print("ğŸ“Š TF-IDF íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
    vectorizer = TfidfVectorizer(
        max_features=1000,
        ngram_range=(1, 2),
        min_df=2,
        max_df=0.8,
        stop_words=None
    )
    
    text_features = vectorizer.fit_transform(texts).toarray()
    
    # íŠ¹ì§• ì €ì¥
    feature_data = {
        'text_features': text_features,
        'vectorizer': vectorizer,
        'texts': texts
    }
    
    if save_path:
        save_path = project_root / save_path if not Path(save_path).is_absolute() else Path(save_path)
        print(f"ğŸ’¾ OCR íŠ¹ì§• ì €ì¥: {save_path}")
        save_path.parent.mkdir(parents=True, exist_ok=True)
        with open(save_path, 'wb') as f:
            pickle.dump(feature_data, f)
    
    print(f"âœ… OCR íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ. í˜•íƒœ: {text_features.shape}")
    return feature_data

def train_one_epoch_multimodal(loader, model, optimizer, loss_fn, device, epoch, use_ocr=False):
    """ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì„ ìœ„í•œ í›ˆë ¨ í•¨ìˆ˜"""
    model.train()
    train_loss = 0
    preds_list = []
    targets_list = []
    
    pbar = tqdm(loader, desc=f'Training Epoch {epoch}')
    for batch_idx, batch_data in enumerate(pbar):
        
        if use_ocr:
            images, text_features, targets = batch_data
            text_features = text_features.to(device)
        else:
            images, targets = batch_data
            text_features = None
        
        images = images.to(device)
        targets = targets.to(device)
        
        optimizer.zero_grad()
        
        if use_ocr:
            outputs = model(images, text_features)
        else:
            outputs = model(images)
            
        loss = loss_fn(outputs, targets)
        loss.backward()
        optimizer.step()
        
        train_loss += loss.item()
        predictions = outputs.argmax(dim=1)
        preds_list.extend(predictions.detach().cpu().numpy())
        targets_list.extend(targets.detach().cpu().numpy())
        
        pbar.set_postfix({'Loss': f'{loss.item():.4f}'})
        
        if batch_idx % 50 == 0:
            log_metrics({
                'batch_loss': loss.item(),
                'batch': epoch * len(loader) + batch_idx
            }, commit=False)
    
    avg_loss = train_loss / len(loader)
    accuracy = accuracy_score(targets_list, preds_list)
    f1 = f1_score(targets_list, preds_list, average='macro')
    
    return {
        'train_loss': avg_loss,
        'train_accuracy': accuracy,
        'train_f1': f1
    }

def train_with_ocr():
    """OCR í†µí•© í›ˆë ¨ í•¨ìˆ˜"""
    
    # OCR ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    use_ocr = OCR_AVAILABLE and EXPERIMENT_CONFIG.get('use_ocr', True)
    
    if use_ocr:
        print(f"ğŸ”¤ OCR ëª¨ë“œ í™œì„±í™” ({OCR_BACKEND})")
    else:
        print("ğŸ–¼ï¸  ì´ë¯¸ì§€ ì „ìš© ëª¨ë“œ (OCR ë¹„í™œì„±í™”)")
    
    # ê¸°ì¡´ ì„¤ì • ë¡œë“œ
    print_configuration()
    
    if not validate_config():
        print("Configuration validation failed.")
        return
    
    device, device_type = setup_training_device()
    dataloader_config = get_dataloader_config(device_type)
    
    # OCR íŠ¹ì§• ì¶”ì¶œ (use_ocr=Trueì¸ ê²½ìš°ì—ë§Œ)
    train_text_features = None
    test_text_features = None
    
    if use_ocr:
        # í›ˆë ¨ ë°ì´í„° OCR ì²˜ë¦¬
        train_ocr_path = "data/ocr_cache/train_ocr_features.pkl"
        train_feature_data = extract_and_cache_ocr_features(
            DATA_CONFIG['train_csv'], 
            DATA_CONFIG['train_dir'], 
            train_ocr_path, 
            backend=OCR_BACKEND
        )
        train_text_features = train_feature_data['text_features']
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° OCR ì²˜ë¦¬
        test_ocr_path = "data/ocr_cache/test_ocr_features.pkl"
        test_feature_data = extract_and_cache_ocr_features(
            DATA_CONFIG['test_csv'], 
            DATA_CONFIG['test_dir'], 
            test_ocr_path, 
            backend=OCR_BACKEND
        )
        test_text_features = test_feature_data['text_features']
    
    # WandB ì´ˆê¸°í™”
    run_name = create_run_name(
        model_name=EXPERIMENT_CONFIG['model_name'],
        experiment_type="ocr_multimodal" if use_ocr else "image_only"
    )
    
    wandb_config = get_wandb_config()
    wandb_config['use_ocr'] = use_ocr
    wandb_config['ocr_backend'] = OCR_BACKEND if use_ocr else None
    
    run = init_wandb(wandb_config, run_name=run_name)
    
    try:
        # ë°ì´í„° ì¤€ë¹„
        train_df = pd.read_csv(DATA_CONFIG['train_csv'])
        train_idx, val_idx = train_test_split(
            range(len(train_df)),
            test_size=DATA_CONFIG['val_split'],
            stratify=train_df['target'],
            random_state=DATA_CONFIG['random_seed']
        )
        
        train_subset = train_df.iloc[train_idx].reset_index(drop=True)
        val_subset = train_df.iloc[val_idx].reset_index(drop=True)
        
        # ì´ë¯¸ì§€ ë³€í™˜
        img_size = EXPERIMENT_CONFIG['img_size']
        train_transform = get_transforms(img_size, is_train=True)
        val_transform = get_transforms(img_size, is_train=False)
        
        # ë°ì´í„°ì…‹ ìƒì„±
        train_dataset = EnhancedImageDataset(
            DATA_CONFIG['train_csv'],
            DATA_CONFIG['train_dir'],
            transform=train_transform,
            is_train=True,
            use_ocr=use_ocr,
            text_features=train_text_features[train_idx] if use_ocr else None
        )
        train_dataset.df = train_subset
        
        val_dataset = EnhancedImageDataset(
            DATA_CONFIG['train_csv'],
            DATA_CONFIG['train_dir'],
            transform=val_transform,
            is_train=True,
            use_ocr=use_ocr,
            text_features=train_text_features[val_idx] if use_ocr else None
        )
        val_dataset.df = val_subset
        
        # ë°ì´í„° ë¡œë” ìƒì„± ì‹œ ë””ë°”ì´ìŠ¤ íƒ€ì…ì— ë”°ë¥¸ ìµœì í™” ì„¤ì • ì‚¬ìš©
        train_loader = DataLoader(
            train_dataset,
            batch_size=EXPERIMENT_CONFIG['batch_size'],
            shuffle=True,
            **dataloader_config  # pin_memoryì™€ num_workers ì„¤ì • ì‚¬ìš©
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=EXPERIMENT_CONFIG['batch_size'],
            shuffle=False,
            **dataloader_config  # pin_memoryì™€ num_workers ì„¤ì • ì‚¬ìš©
        )
        
        # ëª¨ë¸ ìƒì„± (í…ìŠ¤íŠ¸ íŠ¹ì§• ì°¨ì› ë™ì  ì„¤ì •)
        text_dim = train_text_features.shape[1] if use_ocr and train_text_features is not None else 1000
        print(f"ğŸ”¤ í…ìŠ¤íŠ¸ íŠ¹ì§• ì°¨ì›: {text_dim}")
        
        model = MultiModalModel(
            model_name=EXPERIMENT_CONFIG['model_name'],
            num_classes=EXPERIMENT_CONFIG['num_classes'],
            text_feature_dim=text_dim,
            use_ocr=use_ocr
        ).to(device)
        
        # í›ˆë ¨ ì„¤ì •
        loss_fn = nn.CrossEntropyLoss()
        optimizer = Adam(
            model.parameters(), 
            lr=EXPERIMENT_CONFIG['learning_rate'],
            weight_decay=EXPERIMENT_CONFIG['weight_decay']
        )
        scheduler = CosineAnnealingLR(
            optimizer,
            T_max=EXPERIMENT_CONFIG['epochs'],
            eta_min=EXPERIMENT_CONFIG['min_lr']
        )
        
        # í›ˆë ¨ ë£¨í”„
        best_val_f1 = 0.0
        best_epoch = 0
        total_start_time = time.time()
        
        print(f"ğŸš€ OCR í†µí•© í›ˆë ¨ ì‹œì‘ ({EXPERIMENT_CONFIG['epochs']} epochs)...")
        
        for epoch in range(EXPERIMENT_CONFIG['epochs']):
            start_time = time.time()
            
            # í›ˆë ¨
            train_metrics = train_one_epoch_multimodal(
                train_loader, model, optimizer, loss_fn, device, epoch, use_ocr
            )
            
            # ê²€ì¦ (ê¸°ì¡´ í•¨ìˆ˜ ì¬ì‚¬ìš©, OCR ì§€ì› ì¶”ê°€ í•„ìš”)
            val_metrics = validate_one_epoch_multimodal(
                val_loader, model, loss_fn, device, epoch, use_ocr
            )
            
            scheduler.step()
            current_lr = optimizer.param_groups[0]['lr']
            epoch_time = time.time() - start_time
            
            # ë©”íŠ¸ë¦­ ë¡œê¹…
            all_metrics = {
                **train_metrics,
                **val_metrics,
                'learning_rate': current_lr,
                'epoch': epoch,
                'epoch_time': epoch_time
            }
            
            log_metrics(all_metrics)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            print(f"\nğŸ“Š Epoch {epoch+1}/{EXPERIMENT_CONFIG['epochs']}:")
            print(f"  ğŸŸ¢ Train - Loss: {train_metrics['train_loss']:.4f} | "
                  f"F1: {train_metrics['train_f1']:.4f}")
            print(f"  ğŸŸ¡ Valid - Loss: {val_metrics['val_loss']:.4f} | "
                  f"F1: {val_metrics['val_f1']:.4f}")
            
            # ìµœê³  ëª¨ë¸ ì €ì¥
            if val_metrics['val_f1'] > best_val_f1:
                best_val_f1 = val_metrics['val_f1']
                best_epoch = epoch
                
                model_save_path = EXPERIMENT_CONFIG['best_model_path']
                os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
                
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'val_f1': best_val_f1,
                    'config': EXPERIMENT_CONFIG,
                    'use_ocr': use_ocr,
                    'ocr_backend': OCR_BACKEND,
                    'text_feature_dim': text_dim  # í…ìŠ¤íŠ¸ ì°¨ì› ì •ë³´ ì €ì¥
                }, model_save_path)
                
                print(f"  ğŸ† NEW BEST! Val F1: {best_val_f1:.4f}")
        
        # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ (í›ˆë ¨ ì™„ë£Œ í›„)
        if use_ocr:
            print(f"\nğŸ”® OCR í†µí•© ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ì‹œì‘...")
            print(f"ğŸ“Š í›ˆë ¨ ì‹œ í…ìŠ¤íŠ¸ ì°¨ì›: {text_dim}")
            print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì‹œ í…ìŠ¤íŠ¸ ì°¨ì›: {test_text_features.shape[1] if test_text_features is not None else 'None'}")
            
            # ì°¨ì› ì¼ì¹˜ í™•ì¸
            if test_text_features is not None and test_text_features.shape[1] != text_dim:
                print(f"âš ï¸  ì°¨ì› ë¶ˆì¼ì¹˜ ê°ì§€! í›ˆë ¨:{text_dim} vs í…ŒìŠ¤íŠ¸:{test_text_features.shape[1]}")
                print(f"ğŸ”„ í…ŒìŠ¤íŠ¸ íŠ¹ì§•ì„ í›ˆë ¨ ì°¨ì›ì— ë§ì¶° ì¡°ì •í•©ë‹ˆë‹¤...")
                
                # ì°¨ì› ì¡°ì • (íŒ¨ë”© ë˜ëŠ” ìë¥´ê¸°)
                if test_text_features.shape[1] < text_dim:
                    # íŒ¨ë”©ìœ¼ë¡œ ì°¨ì› ëŠ˜ë¦¬ê¸°
                    padding = text_dim - test_text_features.shape[1]
                    test_text_features = np.pad(test_text_features, ((0, 0), (0, padding)), mode='constant')
                    print(f"âœ… íŒ¨ë”©ìœ¼ë¡œ {text_dim}ì°¨ì›ìœ¼ë¡œ ì¡°ì • ì™„ë£Œ")
                else:
                    # ìë¥´ê¸°ë¡œ ì°¨ì› ì¤„ì´ê¸°
                    test_text_features = test_text_features[:, :text_dim]
                    print(f"âœ… ìë¥´ê¸°ë¡œ {text_dim}ì°¨ì›ìœ¼ë¡œ ì¡°ì • ì™„ë£Œ")
            
            predict_with_ocr(model, device, test_text_features)
        else:
            print("\nğŸ–¼ï¸  ì´ë¯¸ì§€ ì „ìš© ëª¨ë¸ - OCR ì˜ˆì¸¡ ìƒëµ")
        
        print(f"\nğŸ‰ OCR í†µí•© í›ˆë ¨ ì™„ë£Œ! ìµœê³  F1: {best_val_f1:.4f}")
        
    except Exception as e:
        print(f"í›ˆë ¨ ì¤‘ ì˜¤ë¥˜: {e}")
        raise
    finally:
        finish_run()

def get_transforms(img_size, is_train=True):
    """ê¸°ì¡´ transform í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ)"""
    if is_train:
        transform = A.Compose([
            A.Resize(height=img_size, width=img_size),
            A.HorizontalFlip(p=0.5),
            A.Rotate(limit=15, p=0.3),
            A.RandomBrightnessContrast(p=0.3),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2(),
        ])
    else:
        transform = A.Compose([
            A.Resize(height=img_size, width=img_size),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2(),
        ])
    
    return transform

def validate_one_epoch_multimodal(loader, model, loss_fn, device, epoch, use_ocr=False):
    """ë©€í‹°ëª¨ë‹¬ ê²€ì¦ í•¨ìˆ˜"""
    model.eval()
    val_loss = 0
    preds_list = []
    targets_list = []
    
    if len(loader) == 0:
        return {
            'val_loss': float('inf'),
            'val_accuracy': 0.0,
            'val_f1': 0.0,
            'predictions': None,
            'targets': None
        }
    
    with torch.no_grad():
        pbar = tqdm(loader, desc=f'Validation Epoch {epoch}')
        for batch_data in pbar:
            
            if use_ocr:
                images, text_features, targets = batch_data
                text_features = text_features.to(device)
            else:
                images, targets = batch_data
                text_features = None
            
            images = images.to(device)
            targets = targets.to(device)
            
            if use_ocr:
                outputs = model(images, text_features)
            else:
                outputs = model(images)
                
            loss = loss_fn(outputs, targets)
            
            val_loss += loss.item()
            predictions = outputs.argmax(dim=1)
            preds_list.extend(predictions.cpu().numpy())
            targets_list.extend(targets.cpu().numpy())
            
            pbar.set_postfix({'Val Loss': f'{loss.item():.4f}'})
    
    avg_loss = val_loss / len(loader)
    accuracy = accuracy_score(targets_list, preds_list)
    f1 = f1_score(targets_list, preds_list, average='macro')
    
    return {
        'val_loss': avg_loss,
        'val_accuracy': accuracy,
        'val_f1': f1,
        'predictions': np.array(preds_list),
        'targets': np.array(targets_list)
    }

def print_configuration():
    """ê¸°ì¡´ ì„¤ì • ì¶œë ¥ í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ)"""
    print("\n" + "="*60)
    print("ğŸš€ CV-Classification Training Configuration")
    print("="*60)
    print(f"ğŸ“‹ Model Settings:")
    print(f"  Model: {EXPERIMENT_CONFIG['model_name']}")
    print(f"  Number of classes: {EXPERIMENT_CONFIG['num_classes']}")
    print(f"  Pretrained: {EXPERIMENT_CONFIG['pretrained']}")
    print(f"\nğŸ“Š Training Settings:")
    print(f"  Image size: {EXPERIMENT_CONFIG['img_size']}")
    print(f"  Batch size: {EXPERIMENT_CONFIG['batch_size']}")
    print(f"  Learning rate: {EXPERIMENT_CONFIG['learning_rate']}")
    print(f"  Epochs: {EXPERIMENT_CONFIG['epochs']}")
    print(f"\nğŸ”¤ OCR Settings:")
    print(f"  OCR Available: {OCR_AVAILABLE}")
    print(f"  OCR Backend: {OCR_BACKEND}")
    print(f"  Use OCR: {EXPERIMENT_CONFIG.get('use_ocr', True) and OCR_AVAILABLE}")
    print("="*60 + "\n")

def predict_with_ocr(model, device, test_text_features):
    """ê³ ê¸‰ OCR í†µí•© ëª¨ë¸ì„ ì‚¬ìš©í•œ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡"""
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„
    img_size = EXPERIMENT_CONFIG['img_size']
    test_transform = get_transforms(img_size, is_train=False)
    
    # test.csv ê²½ë¡œ í™•ì¸ ë° ìˆ˜ì •
    test_csv_path = DATA_CONFIG.get('test_csv', 'data/sample_submission.csv')
    test_dir_path = DATA_CONFIG.get('test_dir', 'data/test')
    
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_csv_path}")
    print(f"ğŸ“ í…ŒìŠ¤íŠ¸ í´ë”: {test_dir_path}")
    print(f"ğŸ”¤ í…ìŠ¤íŠ¸ íŠ¹ì§• ìƒíƒœ: {test_text_features.shape if test_text_features is not None else 'None'}")
    
    test_dataset = EnhancedImageDataset(
        test_csv_path,
        test_dir_path,
        transform=test_transform,
        is_train=False,
        use_ocr=True,
        text_features=test_text_features
    )
    
    dataloader_config = get_dataloader_config(device.type)  # ë””ë°”ì´ìŠ¤ íƒ€ì… ì „ë‹¬
    test_loader = DataLoader(
        test_dataset,
        batch_size=EXPERIMENT_CONFIG['batch_size'],
        shuffle=False,
        **dataloader_config  # pin_memoryì™€ num_workers ì„¤ì • ì‚¬ìš©
    )
    
    # ì˜ˆì¸¡ ìˆ˜í–‰
    model.eval()
    predictions = []
    
    print("\nğŸ”® OCR í†µí•© ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...")
    with torch.no_grad():
        for batch_idx, (images, text_features, _) in enumerate(tqdm(test_loader, desc="OCR ì˜ˆì¸¡")):
            images = images.to(device)
            text_features = text_features.to(device)
            
            outputs = model(images, text_features)
            batch_predictions = outputs.argmax(dim=1).cpu().numpy()
            predictions.extend(batch_predictions)
    
    # ì œì¶œ íŒŒì¼ ìƒì„±
    test_csv_path = DATA_CONFIG.get('test_csv', 'data/sample_submission.csv')
    test_df = pd.read_csv(test_csv_path)
    submission_df = pd.DataFrame({
        'ID': test_df['ID'],
        'target': predictions
    })
    
    # íŒŒì¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    submission_file = f"codes/ocr_submission_{timestamp}.csv"
    os.makedirs(os.path.dirname(submission_file), exist_ok=True)
    submission_df.to_csv(submission_file, index=False)
    
    print(f"ğŸ“„ OCR ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {submission_file}")
    print(f"ğŸ“Š ì´ ì˜ˆì¸¡ ìˆ˜: {len(predictions)}")
    
    # ì˜ˆì¸¡ ë¶„í¬ í‘œì‹œ
    unique, counts = np.unique(predictions, return_counts=True)
    print(f"ğŸ“ˆ OCR ëª¨ë¸ ì˜ˆì¸¡ ë¶„í¬:")
    for class_id, count in zip(unique, counts):
        percentage = count / len(predictions) * 100
        print(f"  Class {class_id}: {count} ({percentage:.1f}%)")
    
    return submission_df

if __name__ == "__main__":
    
    # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    from config import MODEL_PATHS
    EXPERIMENT_CONFIG.update({
        'best_model_path': MODEL_PATHS['best_model'],
        'latest_model_path': MODEL_PATHS['latest_model'],
        'use_ocr': True  # OCR ì‚¬ìš© ì—¬ë¶€
    })
    
    # DRY RUN ëª¨ë“œ
    if len(sys.argv) > 1 and sys.argv[1] == "--dry-run":
        print("ğŸ§ª OCR DRY RUN ëª¨ë“œ")
        print("="*60)
        
        try:
            print("1. OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸...")
            if OCR_AVAILABLE:
                print(f"   âœ… OCR ë°±ì—”ë“œ: {OCR_BACKEND}")
            else:
                print("   âš ï¸  OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ (ì´ë¯¸ì§€ ì „ìš© ëª¨ë“œ)")
            
            print("\n2. í™˜ê²½ ê²€ì¦...")
            if validate_config():
                print("   âœ… ì„¤ì • ê²€ì¦ ì„±ê³µ")
            
            print("\n3. ë””ë°”ì´ìŠ¤ ì„¤ì •...")
            device, device_type = setup_training_device()
            print(f"   âœ… ë””ë°”ì´ìŠ¤: {device}")
            
            print("\nğŸ‰ DRY RUN ì™„ë£Œ! OCR í†µí•© í›ˆë ¨ ì¤€ë¹„ ì™„ë£Œ")
            
        except Exception as e:
            print(f"\nâŒ DRY RUN ì‹¤íŒ¨: {e}")
            sys.exit(1)
    else:
        train_with_ocr()

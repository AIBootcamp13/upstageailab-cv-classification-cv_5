"""
macOS ìµœì í™”ëœ ë””ë°”ì´ìŠ¤ ìœ í‹¸ë¦¬í‹°
Apple Silicon (M1/M2/M3) GPU ê°€ì† ì§€ì›
"""

import torch
import platform
import psutil

def get_optimal_device():
    """
    í¬ë¡œìŠ¤ í”Œë«í¼ ìµœì í™”ëœ ë””ë°”ì´ìŠ¤ ê°ì§€
    - ëª¨ë“  í”Œë«í¼: GPU ìµœìš°ì„ ! (CUDA ë˜ëŠ” MPS) â†’ CPU
    """
    
    # ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸
    system_info = {
        'platform': platform.system(),
        'machine': platform.machine(),
        'processor': platform.processor()
    }
    
    print(f"ğŸ–¥ï¸  ì‹œìŠ¤í…œ: {system_info['platform']} {system_info['machine']}")
    
    # ì „ì²´ í”Œë«í¼ì—ì„œ GPU ìµœìš°ì„ 
    
    # 1. CUDA í™•ì¸ (Ubuntu/Linux/Windows - NVIDIA GPU)
    if torch.cuda.is_available():
        device = torch.device('cuda')
        gpu_name = torch.cuda.get_device_name(0)
        print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥ - GPU: {gpu_name}")
        return device, "CUDA"
    
    # 2. MPS í™•ì¸ (macOS - Apple Silicon)
    elif torch.backends.mps.is_available():
        device = torch.device('mps')
        print("âœ… MPS (Apple Silicon GPU) ì‚¬ìš© ê°€ëŠ¥ - GPU ê°€ì† í™œì„±í™”")
        return device, "MPS"
    
    # 3. CPU fallback (ëª¨ë“  í”Œë«í¼ ê³µí†µ - GPU ì—†ì„ ë•Œë§Œ)
    else:
        device = torch.device('cpu')
        print("âš ï¸  GPU ì‚¬ìš© ë¶ˆê°€ - CPU ì‚¬ìš©")
        
        # í”Œë«í¼ë³„ CPU ìµœì í™” ì•ˆë‚´
        if system_info['platform'] == 'Darwin':
            print("ğŸ’¡ macOS CPU ìµœì í™” í™œì„±í™”")
        elif system_info['platform'] == 'Linux':
            print("ğŸ’¡ Linux CPU ìµœì í™” í™œì„±í™”")
            
        return device, "CPU"

def check_gpu_memory(device):
    """GPU ë©”ëª¨ë¦¬ ì •ë³´ í™•ì¸ (í¬ë¡œìŠ¤ í”Œë«í¼)"""
    try:
        if device.type == 'cuda':
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            allocated_memory = torch.cuda.memory_allocated(0) / 1e9
            print(f"ğŸš€ CUDA ë©”ëª¨ë¦¬: {total_memory:.1f} GB (ì‚¬ìš©ì¤‘: {allocated_memory:.1f} GB)")
            
        elif device.type == 'mps':
            # macOS í†µí•© ë©”ëª¨ë¦¬ ì •ë³´
            total_memory = psutil.virtual_memory().total / 1e9
            print(f"ğŸ MPS ë©”ëª¨ë¦¬: {total_memory:.1f} GB (í†µí•© ë©”ëª¨ë¦¬)")
            
        else:
            # CPU ë©”ëª¨ë¦¬ ì •ë³´ (Linux/Ubuntu/macOS ê³µí†µ)
            memory_info = psutil.virtual_memory()
            total_memory = memory_info.total / 1e9
            available_memory = memory_info.available / 1e9
            print(f"ğŸ–¥ï¸  ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {total_memory:.1f} GB (ì‚¬ìš©ê°€ëŠ¥: {available_memory:.1f} GB)")
            
    except Exception as e:
        print(f"âš ï¸  ë©”ëª¨ë¦¬ ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {e}")

def setup_training_device():
    """í•™ìŠµìš© ë””ë°”ì´ìŠ¤ ì„¤ì • ë° ìµœì í™” (í¬ë¡œìŠ¤ í”Œë«í¼)"""
    
    device, device_type = get_optimal_device()
    
    # ë””ë°”ì´ìŠ¤ë³„ ìµœì í™” ì„¤ì •
    if device_type == "MPS":
        # MPS ìµœì í™” ì„¤ì • (macOS)
        try:
            # MPS ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œë„ (ìˆëŠ” ê²½ìš°ë§Œ)
            if hasattr(torch.backends.mps, 'empty_cache'):
                torch.backends.mps.empty_cache()
            elif hasattr(torch.mps, 'empty_cache'):
                torch.mps.empty_cache()
        except (AttributeError, RuntimeError):
            # MPS ë©”ëª¨ë¦¬ ì •ë¦¬ê°€ ì§€ì›ë˜ì§€ ì•ŠëŠ” ê²½ìš° ë¬´ì‹œ
            pass
        print("ğŸ Apple Silicon ìµœì í™” ì„¤ì • ì ìš©")
        
    elif device_type == "CUDA":
        # CUDA ìµœì í™” ì„¤ì • (Ubuntu/Linux/Windows)
        try:
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False  # ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´
            print("ğŸš€ CUDA ìµœì í™” ì„¤ì • ì ìš©")
        except Exception as e:
            print(f"âš ï¸  CUDA ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
        
    else:
        # CPU ìµœì í™” ì„¤ì • (ëª¨ë“  í”Œë«í¼)
        try:
            # CPU ì½”ì–´ ìˆ˜ì— ë”°ë¥¸ ìŠ¤ë ˆë“œ ìˆ˜ ì¡°ì •
            import multiprocessing
            num_cores = multiprocessing.cpu_count()
            optimal_threads = min(num_cores, 8)  # ìµœëŒ€ 8ê°œë¡œ ì œí•œ
            torch.set_num_threads(optimal_threads)
            print(f"ğŸ–¥ï¸  CPU ìµœì í™” ì„¤ì • ì ìš© (ìŠ¤ë ˆë“œ: {optimal_threads}/{num_cores})")
        except Exception as e:
            print(f"âš ï¸  CPU ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
    
    # ë©”ëª¨ë¦¬ ì •ë³´ ì¶œë ¥
    check_gpu_memory(device)
    
    return device, device_type

def get_dataloader_config(device_type):
    """ë””ë°”ì´ìŠ¤ íƒ€ì…ì— ë”°ë¥¸ DataLoader ìµœì í™” ì„¤ì • (í¬ë¡œìŠ¤ í”Œë«í¼)"""
    
    import multiprocessing
    num_cores = multiprocessing.cpu_count()
    
    if device_type == "MPS":
        # Apple Silicon ìµœì í™” (macOS)
        return {
            "pin_memory": False,  # MPSëŠ” pin_memory ë¶ˆí•„ìš”
            "num_workers": 0,     # MPSëŠ” ë©€í‹°í”„ë¡œì„¸ì‹± ì´ìŠˆ ë°©ì§€
        }
    elif device_type == "CUDA":
        # CUDA ìµœì í™” (Ubuntu/Linux/Windows)
        optimal_workers = min(num_cores // 2, 8)  # ì½”ì–´ ìˆ˜ì˜ ì ˆë°˜, ìµœëŒ€ 8
        return {
            "pin_memory": True,   # CUDAëŠ” pin_memory íš¨ê³¼ì 
            "num_workers": optimal_workers,  # ë³‘ë ¬ ë°ì´í„° ë¡œë”©
        }
    else:
        # CPU ìµœì í™” (ëª¨ë“  í”Œë«í¼)
        optimal_workers = min(num_cores // 4, 4)  # CPUì—ì„œëŠ” ì ë‹¹í•œ ìˆ˜ì¤€
        return {
            "pin_memory": False,  # CPUëŠ” pin_memory ë¶ˆí•„ìš”
            "num_workers": optimal_workers,
        }

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_device():
    """ë””ë°”ì´ìŠ¤ í…ŒìŠ¤íŠ¸"""
    print("ğŸ” ë””ë°”ì´ìŠ¤ ê°ì§€ ë° í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    try:
        device, device_type = setup_training_device()
    except Exception as e:
        print(f"âš ï¸  ë””ë°”ì’°ìŠ¤ ì„¤ì • ì˜¤ë¥˜: {e}")
        print("ğŸ“‹ CPU ëª¨ë“œë¡œ fallback...")
        device = torch.device('cpu')
        device_type = "CPU"
    
    print(f"\nğŸ¯ ìµœì¢… ì„ íƒëœ ë””ë°”ì´ìŠ¤: {device} ({device_type})")
    
    # ê°„ë‹¨í•œ í…ì„œ ì—°ì‚° í…ŒìŠ¤íŠ¸
    try:
        print("\nğŸ§ª ë””ë°”ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ì¤‘...")
        test_tensor = torch.randn(1000, 1000).to(device)
        result = torch.matmul(test_tensor, test_tensor)
        print("âœ… í…ì„œ ì—°ì‚° í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        if device_type == "CUDA":
            memory_used = torch.cuda.memory_allocated(0) / 1e6
            print(f"ğŸ“Š GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used:.1f} MB")
            
    except Exception as e:
        print(f"âŒ ë””ë°”ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ CPUë¡œ fallback ê¶Œì¥")
        device = torch.device('cpu')
        device_type = "CPU"
    
    # DataLoader ì„¤ì • ê¶Œì¥ì‚¬í•­
    dataloader_config = get_dataloader_config(device_type)
    print(f"\nâš™ï¸  ê¶Œì¥ DataLoader ì„¤ì •:")
    for key, value in dataloader_config.items():
        print(f"   {key}: {value}")
    
    return device, device_type

if __name__ == "__main__":
    test_device()
